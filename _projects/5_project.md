---
layout: page
title: Efficient Self-Supervised Neural Architecture Search
description: Crafting deep learning architectures efficiently with self-supervised neural architecture search
img: assets/img/publication_preview/nas.png
importance: 5
category: Undergraduate Projects
---

 This project aims to optimize the design of deep neural networks by automating the search for the best-performing architectures. Traditional deep learning models require extensive manual tuning and significant computational resources to achieve optimal performance. This project leverages the concepts of Neural Architecture Search (NAS) and self-supervised learning to create an efficient method for discovering high-performing neural networks. By using techniques such as Differentiable Architecture Search (DARTS) and incorporating self-supervision through methods like the Barlow Twins, the project addresses the challenges of long search times, high processor needs, and the need for large labeled datasets. The developed methodology has been tested on various datasets, including CIFAR-10, demonstrating its ability to balance time, accuracy, and computational efficiency. This approach is particularly beneficial for tasks where labeled data is scarce and highlights the potential for improved transferability to other tasks through self-supervision.

 Code and Report will be available soon!
